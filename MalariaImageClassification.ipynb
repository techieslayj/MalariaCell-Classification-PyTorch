{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "######## AJ IGLESIAS #########\n",
    "##############################\n",
    "\n",
    "### Malaria Detection Neural Network ###\n",
    "### PyTorch Implementation ###\n",
    "\n",
    "##########Import libraries for network ##############\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transforms for training, validation and testing datasets#\n",
    "\n",
    "#Train transform\n",
    "trainTransforms = transforms.Compose([transforms.RandomRotation(30), #rotate image 30 degrees\n",
    "                                     transforms.RandomResizedCrop(224), #crop to image size of 224 x 224\n",
    "                                     transforms.RandomVerticalFlip(), #flip image vertically\n",
    "                                     transforms.ToTensor(), #convert to tensor\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])]) #Normalized mini-batches for the images because we use a pre-trained model (ResNet50) which expects input images normalized in this way\n",
    "\n",
    "#Test transform\n",
    "testTransforms = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224), #crops image into square crop of size 224 x 224\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "#Validation transform\n",
    "validationTransforms = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'cell_images/' # data within the directory hence a very simple path\n",
    "trainData = datasets.ImageFolder(images, transform = trainTransforms) #data loader that arranges the malaria cell images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of training set to use as validation set\n",
    "validSize = 0.3 #30% of training set to use for validation\n",
    "test_size = 0.1\n",
    "\n",
    "#make Float tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#training indices that will be used for validation\n",
    "num_Train = len(trainData)\n",
    "indices = list(range(num_Train))\n",
    "\n",
    "#Shuffle\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "#Splits\n",
    "validSplit = int(np.floor((validSize) * num_Train))\n",
    "testSplit = int(np.floor((validSize + test_size) * num_Train))\n",
    "\n",
    "#index per validation, test, and training\n",
    "validInd, testInd, trainInd = indices[:validSplit], indices[validSplit:testSplit], indices[testSplit:]\n",
    "\n",
    "#lets see lengths\n",
    "print(len(validInd), len(testInd), len(trainInd))\n",
    "\n",
    "#define Samplers for obtaining training and validation batches\n",
    "trainSampler = SubsetRandomSampler(trainInd)\n",
    "testSampler = SubsetRandomSampler(testInd)\n",
    "validSampler = SubsetRandomSampler(validInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare combination of dataset and samplers \n",
    "\n",
    "#dataLoaders gives us wrapper access and querying abilities\n",
    "trainLoad = torch.utils.data.DataLoader(trainData, batch_size = 64, sampler = trainSampler)\n",
    "testLoad = torch.utils.data.DataLoader(trainData, batch_size = 10, sampler = testSampler)\n",
    "validLoad = torch.utils.data.DataLoader(trainData, batch_size = 64, sampler = validSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the length of the test data loader\n",
    "len(testLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pretrained resnet50 for our model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "\n",
    "fcParameters = model.fc.parameters()\n",
    "\n",
    "for param in fcParameters:\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the network\n",
    "def trainNet(epochs, model, optimizer, criterion):\n",
    "    validLoss = np.inf\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        #initialize training and valid loss set to 0.0 so each time it goes through network loss is reset to 0.0\n",
    "        trainLoss = 0.0\n",
    "        validLoss = 0.0\n",
    "        \n",
    "        #Model Training#\n",
    "        model.train()\n",
    "        for batchIndex, (data, target) in enumerate(trainLoad):\n",
    "            \n",
    "            #default weights to 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            #calc Loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            #back propogation\n",
    "            loss.backward()\n",
    "            \n",
    "            #gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            trainLoss = trainLoss + ((1 / (batchIndex + 1)) * (loss.data - trainLoss))\n",
    "            \n",
    "            if batchIndex % 100 == 0:\n",
    "                print('Epoch %d, Batch %d, Loss: %.6f' % (epoch, batchIndex + 1, trainLoss))\n",
    "                \n",
    "            \n",
    "        #evaluate and validate model\n",
    "        \n",
    "        model.eval()\n",
    "        for batchIndex, (data, target) in enumerate(validLoad):\n",
    "            \n",
    "            #Average validation loss\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            validLoss = validLoss + ((1 / (batchIndex + 1)) * (loss.data - validLoss))\n",
    "            \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, trainLoss, validLoss))\n",
    "\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up our optimizer and criterion\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr = 0.001) #establish learning rate as 0.0001\n",
    "criterion = nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "#Lets run the model for 4 epochs due to computer limitations\n",
    "trainNet(4, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test the trained model for accuracy###\n",
    "def testMod(model, criterion):\n",
    "    #Capture loss and accuracy\n",
    "    testLoss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    for batchIndex, (data, target) in enumerate(testLoad):\n",
    "        #forward function pass\n",
    "        output = model(data)\n",
    "        \n",
    "        #calc loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        #Update average test loss\n",
    "        testLoss = testLoss + ((1 / (batchIndex + 1)) * (loss.data - testLoss))\n",
    "        \n",
    "        #establish predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        #compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).numpy())\n",
    "        total += data.size(0)\n",
    "        \n",
    "    print('Test Loss: {:.6f}\\n'.format(testLoss))\n",
    "    \n",
    "    #Print algorithm accuracy\n",
    "    print('\\n Test Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMod(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "def inputImage(image):\n",
    "    image = Image.open(image)\n",
    "    predictTransform = transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, .406],\n",
    "                                                              [0.229, 0.224, 0.225])])\n",
    "    image = predictTransform(image)[:3,:,:].unsqueeze(0)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMalaria(model, class_name, image):\n",
    "    #function to return the predicted malaria cell\n",
    "    image = inputImage(image)\n",
    "    \n",
    "    model.eval()\n",
    "    index = torch.argmax(model(image))\n",
    "    return className[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run these two functions above for prediction and give an idea of the cell we are looking at \n",
    "className = ['Parasitized', 'Uninfected']\n",
    "infected = np.array(glob('cell_images/Parasitized/*'))\n",
    "uninfected = np.array(glob('cell_images/Uninfected/*'))\n",
    "\n",
    "for i in range(5):\n",
    "    imagePath = infected[i]\n",
    "    image = Image.open(imagePath)\n",
    "    if predictMalaria(model, className, imagePath) == 'Parasitized':\n",
    "        print('Parasitized')\n",
    "    else:\n",
    "        print('Uninfected')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    imagePath = uninfected[i]\n",
    "    image = Image.open(imagePath)\n",
    "    if predictMalaria(model, className, imagePath) == 'Uninfected':\n",
    "        print('Uninfected')\n",
    "    else:\n",
    "        print('Parasitized')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
